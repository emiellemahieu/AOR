{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AOR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emiellemahieu/AOR/blob/master/AOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tCs-RPTkHBx",
        "colab_type": "text"
      },
      "source": [
        "### Code Master Dissertation:  'The Art of Roughness, a fractal and neural network combination approach to market risk measurement'\n",
        "\n",
        " This code uses standard parametric and non-parametric approaches to estimate Value-at-risk (VaR) and combines them in a neural net                                                   with the purpose to: \n",
        " \n",
        " (1) Reduce the bias of the methods by combining them (Inui, Kijama, Itano, 2003)                                      \n",
        " (2) Explicitly tell the machine to learn a combination that minimizes the exceptions in financial loss (Kupiec, Christoffersen, 1998)                                          \n",
        " (3) Investigate the use of fractal complexity measures for this combination given the intimate relationship between fractional Brownian motion (fBM) (Mandelbrot,        Van Ness 1968 & Gatheral, 2014) and the generalization of standard assumptions in the classical models.                                            \n",
        "\n",
        "SEE MAIN TEXT      [link text](https://)                                      \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK57W89lbWGZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "### CODE MASTER DISSERTATION:                                                                                                                        ###\n",
        "###\" The Art of Roughness:                                                                                                                           ###\n",
        "###  A fractal and neural network combination approach to market risk measurement\"                                                                   ###\n",
        "###  2019               Emiel Lemahieu                                                                                                               ###\n",
        "########################################################################################################################################################\n",
        "########################################################################################################################################################\n",
        "### This code uses classical parametric, non-parametric and Monte Carlo                                                                              ###\n",
        "### approaches to estimate Value-at-risk (VaR) and combines them in a neural net                                                                     ###\n",
        "### with the purpose to: (1) Reduce the bias of the methods by combining them (Inui, Kijama, Itano, 2003)                                            ###\n",
        "###                      (2) Explicitly tell the machine to learn a combination                                                                      ###\n",
        "###                          that minimizes the exceptions in financial loss (Kupiec, Christoffersen, 1998)                                          ###\n",
        "###                      (3) Investigate the use of fractal complexity measures for this combination                                                 ###\n",
        "###                          given the intimate relationship between fractional Brownian motion (fBM) (Mandelbrot, Van Ness 1968 & Gatheral, 2014)   ###\n",
        "###                          and the generalization of standard assumptions in the classical models.                                                 ###\n",
        "###                                                                                                                                                  ###\n",
        "### SEE MAIN TEXT                                                                                                                                    ###\n",
        "########################################################################################################################################################\n",
        "\n",
        "### Required Installs\n",
        "\n",
        "!pip install git+https://github.com/bashtage/arch.git #--install-option \"--no-binary\"\n",
        "!pip install pandas_datareader\n",
        "!pip install fbm\n",
        "!pip install --upgrade setuptools\n",
        "!pip install git+https://github.com/neuropsychology/NeuroKit.py.git #--install-option \"--no-binary\"\n",
        "!pip install git+https://github.com/quandl/quandl-python.git\n",
        "!pip install suds\n",
        "!pip install pydatastream\n",
        "\n",
        "################################################################################\n",
        "### CURRENT COMMENTS / ROOM FOR IMPROVEMENT:                               #####\n",
        "################################################################################\n",
        "\n",
        "# (1) Multiprocessing for loops? \n",
        "# (2) What clusters to run it faster?\n",
        "# (3) Still have to (properly) introduce Kupiec in loss function\n",
        "# (4) Monte Carlo???\n",
        "\n",
        "\n",
        "### df.fillna(df.mean(), inplace=True) !!!!!!!\n",
        "\n",
        "# CHECK TICKERLIST in DO\n",
        "# Genetics currently OFF to work on loss ftion\n",
        "# Evaluate Vol options (GARCH/EGARCH/HARCH normal/t/skewt/ged)\n",
        "# Plots for Vol are turned off !\n",
        "# Shuffle the training set --> dubble check\n",
        "# MCS??? -> TO BE DONE\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### NECESSARY IMPORTS & PRESETS ################################################\n",
        "################################################################################\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
        "from numpy.random import randn\n",
        "import math\n",
        "import io\n",
        "from scipy.stats import norm\n",
        "from IPython import display\n",
        "from google.colab import files\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "from fbm import FBM\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:f}'.format\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "#mpl.rcParams['figure.dpi'] = 600\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "from pandas_datareader import data, wb\n",
        "from google.colab import files\n",
        "import quandl\n",
        "from pydatastream import Datastream\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "import random\n",
        "from keras.datasets import mnist, cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from arch import arch_model\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "print(tf.__version__)\n",
        "plt.ioff()\n",
        "\n",
        "################################################################################\n",
        "###        GENETIC ALGORITHMS          #########################################\n",
        "################################################################################\n",
        "\n",
        "\"\"\"\n",
        "Class that holds a genetic algorithm for evolving a network.\n",
        "Credit:\n",
        "    A lot of this code was originally inspired by:\n",
        "    http://lethain.com/genetic-algorithms-cool-name-damn-simple/\n",
        "\"\"\"\n",
        "\n",
        "# Helper: Early stopping.\n",
        "early_stopper = EarlyStopping(patience=20)\n",
        "\n",
        "def compile_model(network, train_data, test_data, train_labels, test_labels, dataframe,nh,n):\n",
        "    \"\"\"Compile a sequential model.\n",
        "    Args:\n",
        "        network (dict): the parameters of the network\n",
        "    Returns:\n",
        "        a compiled network.\n",
        "    \"\"\"\n",
        "    # Get our network parameters.\n",
        "    nb_layers = network['nb_layers']\n",
        "    nb_neurons = network['nb_neurons']\n",
        "    activation = network['activation']\n",
        "    optimizer = network['optimizer']\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add each layer.\n",
        "    for i in range(nb_layers):\n",
        "\n",
        "        # Need input shape for first layer.\n",
        "        if i == 0:\n",
        "            model.add(Dense(nb_neurons, activation=activation,input_shape=(train_data.shape[1],)))\n",
        "        else:\n",
        "            model.add(Dense(nb_neurons, activation=activation))\n",
        "\n",
        "        model.add(Dropout(0.2))  # hard-coded dropout\n",
        "    \n",
        "    n=n\n",
        "    df = dataframe\n",
        "    nh = nh\n",
        "    close='CLOSE'\n",
        "    q=max(200,round(len(df[close])/5))\n",
        "    n = len(df[close])-q\n",
        "    n = len(df[close])-q\n",
        "    \n",
        "    def myloss(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      errorGreater = K.cast(K.greater(returntensor,y_pred), 'float32')\n",
        "      c=tf.reduce_sum(errorGreater)\n",
        "      errorGreater2 = K.cast(K.greater(y_pred,-10000),'float32')\n",
        "      total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      #customloss=10*(cl-p)*(cl-p)+customloss\n",
        "      customloss=penalty*(cl-(1-p))*(cl-(1-p))+customloss\n",
        "      return customloss\n",
        "    \n",
        "    \n",
        "    model.compile(loss=myloss, optimizer=optimizer,\n",
        "                  metrics=['accuracy','mae'])\n",
        "    return model\n",
        "  \n",
        "def plot_history(history):\n",
        "    plt.subplot(111)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean Abs Error')\n",
        "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
        "             label='Train Loss')\n",
        "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
        "             label = 'Val loss')\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 0.0250])\n",
        "    plt.show()\n",
        "    \n",
        "def get_data(train_data, test_data,train_labels,test_labels):\n",
        "    \"\"\"Retrieve the dataset and process the data.\"\"\"\n",
        "    # Set defaults.\n",
        "    #nbclasses = 10\n",
        "    batch_size = 64\n",
        "    #input_shape = (3072,)\n",
        "\n",
        "    # Get the data.\n",
        "    x_train = train_data\n",
        "    x_test = test_data\n",
        "    y_train = train_labels\n",
        "    y_test = test_labels\n",
        "    \n",
        "    #return (nb_classes, batch_size, input_shape, x_train, x_test, y_train, y_test)\n",
        "    return (batch_size,x_train, x_test, y_train, y_test)\n",
        "\n",
        "def train_and_score(network, dataset, train_data, test_data,train_labels,test_labels, dataframe,nh,n):\n",
        "    \"\"\"Train the model, return test loss.\n",
        "    Args:\n",
        "        network (dict): the parameters of the network\n",
        "        dataset (str): Dataset to use for training/evaluating\n",
        "    \"\"\"\n",
        "    if dataset == 'thesis':\n",
        "        #nb_classes, batch_size, input_shape, x_train, \\\n",
        "        \n",
        "            batch_size, x_train, x_test, y_train, y_test = get_data(train_data,test_data,train_labels,test_labels)\n",
        "    #elif dataset == 'mnist':\n",
        "     #   nb_classes, batch_size, input_shape, x_train, \\\n",
        "      #      x_test, y_train, y_test = get_mnist()\n",
        "\n",
        "    model = compile_model(network, train_data, test_data, train_labels, test_labels, dataframe,nh,n)\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1000,  # using early stopping, so no real limit\n",
        "              verbose=0,\n",
        "              validation_data=(x_test, y_test),\n",
        "              callbacks=[early_stopper])\n",
        "    \n",
        "    train_predictions=model.predict(train_data)\n",
        "    test_predictions=model.predict(test_data)\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    \n",
        "    #return train_predictions, test_predictions, history, score[1]  # 1 is accuracy. 0 is loss.\n",
        "    return train_predictions, test_predictions, history, score[0]  # 1 is accuracy. 0 is loss.\n",
        "  \n",
        "class Optimizer():\n",
        "    \"\"\"Class that implements genetic algorithm for MLP optimization.\"\"\"\n",
        "\n",
        "    def __init__(self, nn_param_choices, retain=0.4,\n",
        "                 random_select=0.1, mutate_chance=0.2):\n",
        "        \"\"\"Create an optimizer.\n",
        "        Args:\n",
        "            nn_param_choices (dict): Possible network paremters\n",
        "            retain (float): Percentage of population to retain after\n",
        "                each generation\n",
        "            random_select (float): Probability of a rejected network\n",
        "                remaining in the population\n",
        "            mutate_chance (float): Probability a network will be\n",
        "                randomly mutated\n",
        "        \"\"\"\n",
        "        self.mutate_chance = mutate_chance\n",
        "        self.random_select = random_select\n",
        "        self.retain = retain\n",
        "        self.nn_param_choices = nn_param_choices\n",
        "\n",
        "    def create_population(self, count):\n",
        "        \"\"\"Create a population of random networks.\n",
        "        Args:\n",
        "            count (int): Number of networks to generate, aka the\n",
        "                size of the population\n",
        "        Returns:\n",
        "            (list): Population of network objects\n",
        "        \"\"\"\n",
        "        pop = []\n",
        "        for _ in range(0, count):\n",
        "            # Create a random network.\n",
        "            network = Network(self.nn_param_choices)\n",
        "            network.create_random()\n",
        "\n",
        "            # Add the network to our population.\n",
        "            pop.append(network)\n",
        "\n",
        "        return pop\n",
        "\n",
        "    @staticmethod\n",
        "    def fitness(network):\n",
        "        \"\"\"Return the accuracy, which is our fitness function.\"\"\"\n",
        "        #return network.accuracy\n",
        "        return 1/network.accuracy\n",
        "\n",
        "    def grade(self, pop):\n",
        "        \"\"\"Find average fitness for a population.\n",
        "        Args:\n",
        "            pop (list): The population of networks\n",
        "        Returns:\n",
        "            (float): The average accuracy of the population\n",
        "        \"\"\"\n",
        "        summed = reduce(add, (self.fitness(network) for network in pop))\n",
        "        return summed / float((len(pop)))\n",
        "\n",
        "    def breed(self, mother, father):\n",
        "        \"\"\"Make two children as parts of their parents.\n",
        "        Args:\n",
        "            mother (dict): Network parameters\n",
        "            father (dict): Network parameters\n",
        "        Returns:\n",
        "            (list): Two network objects\n",
        "        \"\"\"\n",
        "        children = []\n",
        "        for _ in range(2):\n",
        "\n",
        "            child = {}\n",
        "\n",
        "            # Loop through the parameters and pick params for the kid.\n",
        "            for param in self.nn_param_choices:\n",
        "                child[param] = random.choice(\n",
        "                    [mother.network[param], father.network[param]]\n",
        "                )\n",
        "\n",
        "            # Now create a network object.\n",
        "            network = Network(self.nn_param_choices)\n",
        "            network.create_set(child)\n",
        "\n",
        "            # Randomly mutate some of the children.\n",
        "            if self.mutate_chance > random.random():\n",
        "                network = self.mutate(network)\n",
        "\n",
        "            children.append(network)\n",
        "\n",
        "        return children\n",
        "\n",
        "    def mutate(self, network):\n",
        "        \"\"\"Randomly mutate one part of the network.\n",
        "        Args:\n",
        "            network (dict): The network parameters to mutate\n",
        "        Returns:\n",
        "            (Network): A randomly mutated network object\n",
        "        \"\"\"\n",
        "        # Choose a random key.\n",
        "        mutation = random.choice(list(self.nn_param_choices.keys()))\n",
        "\n",
        "        # Mutate one of the params.\n",
        "        network.network[mutation] = random.choice(self.nn_param_choices[mutation])\n",
        "\n",
        "        return network\n",
        "\n",
        "    def evolve(self, pop):\n",
        "        \"\"\"Evolve a population of networks.\n",
        "        Args:\n",
        "            pop (list): A list of network parameters\n",
        "        Returns:\n",
        "            (list): The evolved population of networks\n",
        "        \"\"\"\n",
        "        # Get scores for each network.\n",
        "        graded = [(self.fitness(network), network) for network in pop]\n",
        "\n",
        "        # Sort on the scores.\n",
        "        graded = [x[1] for x in sorted(graded, key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "        # Get the number we want to keep for the next gen.\n",
        "        retain_length = int(len(graded)*self.retain)\n",
        "\n",
        "        # The parents are every network we want to keep.\n",
        "        parents = graded[:retain_length]\n",
        "\n",
        "        # For those we aren't keeping, randomly keep some anyway.\n",
        "        for individual in graded[retain_length:]:\n",
        "            if self.random_select > random.random():\n",
        "                parents.append(individual)\n",
        "\n",
        "        # Now find out how many spots we have left to fill.\n",
        "        parents_length = len(parents)\n",
        "        desired_length = len(pop) - parents_length\n",
        "        children = []\n",
        "\n",
        "        # Add children, which are bred from two remaining networks.\n",
        "        while len(children) < desired_length:\n",
        "\n",
        "            # Get a random mom and dad.\n",
        "            male = random.randint(0, parents_length-1)\n",
        "            female = random.randint(0, parents_length-1)\n",
        "\n",
        "            # Assuming they aren't the same network...\n",
        "            if male != female:\n",
        "                male = parents[male]\n",
        "                female = parents[female]\n",
        "\n",
        "                # Breed them.\n",
        "                babies = self.breed(male, female)\n",
        "\n",
        "                # Add the children one at a time.\n",
        "                for baby in babies:\n",
        "                    # Don't grow larger than desired length.\n",
        "                    if len(children) < desired_length:\n",
        "                        children.append(baby)\n",
        "\n",
        "        parents.extend(children)\n",
        "\n",
        "        return parents\n",
        "      \n",
        "      \n",
        "      \n",
        "\"\"\"Class that represents the network to be evolved.\"\"\"\n",
        "import random\n",
        "import logging\n",
        "\n",
        "\n",
        "class Network():\n",
        "    \"\"\"Represent a network and let us operate on it.\n",
        "    Currently only works for an MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nn_param_choices=None):\n",
        "        \"\"\"Initialize our network.\n",
        "        Args:\n",
        "            nn_param_choices (dict): Parameters for the network, includes:\n",
        "                nb_neurons (list): [64, 128, 256]\n",
        "                nb_layers (list): [1, 2, 3, 4]\n",
        "                activation (list): ['relu', 'elu']\n",
        "                optimizer (list): ['rmsprop', 'adam']\n",
        "        \"\"\"\n",
        "        self.accuracy = 0.\n",
        "        self.history={}\n",
        "        self.test_predictions=None # Change back to [] \n",
        "        self.train_predictions=None\n",
        "        self.nn_param_choices = nn_param_choices\n",
        "        self.network = {}  # (dic): represents MLP network parameters\n",
        "\n",
        "    def create_random(self):\n",
        "        \"\"\"Create a random network.\"\"\"\n",
        "        for key in self.nn_param_choices:\n",
        "            self.network[key] = random.choice(self.nn_param_choices[key])\n",
        "\n",
        "    def create_set(self, network):\n",
        "        \"\"\"Set network properties.\n",
        "        Args:\n",
        "            network (dict): The network parameters\n",
        "        \"\"\"\n",
        "        self.network = network\n",
        "\n",
        "    def train(self, dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n):\n",
        "        \"\"\"Train the network and record the accuracy.\n",
        "        Args:\n",
        "            dataset (str): Name of dataset to use.\n",
        "        \"\"\"\n",
        "        if self.accuracy == 0.:\n",
        "            self.train_predictions, self.test_predictions, self.history, self.accuracy = train_and_score(self.network, dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n)\n",
        "\n",
        "    def print_network(self):\n",
        "        \"\"\"Print out a network.\"\"\"\n",
        "        print(self.network)\n",
        "        #print(\"Network accuracy: %.2f%%\" % (self.accuracy * 100))\n",
        "        print(\"Network score: %.2f%\" % (self.accuracy * 100))\n",
        "        #plot_history(self.history)\n",
        "    \n",
        "    def network_test_predictions(self):\n",
        "      return self.test_predictions\n",
        "    \n",
        "    def network_train_predictions(self):\n",
        "      return self.train_predictions\n",
        "        \n",
        "import logging\n",
        "#from optimizer import Optimizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setup logging.\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
        "    level=logging.DEBUG,\n",
        "    filename='log.txt'\n",
        ")\n",
        "\n",
        "def train_networks(networks, dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n):\n",
        "    \"\"\"Train each network.\n",
        "    Args:\n",
        "        networks (list): Current population of networks\n",
        "        dataset (str): Dataset to use for training/evaluating\n",
        "    \"\"\"\n",
        "    pbar = tqdm(total=len(networks))\n",
        "    for network in networks:\n",
        "        network.train(dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "\n",
        "def get_average_accuracy(networks):\n",
        "    \"\"\"Get the average accuracy for a group of networks.\n",
        "    Args:\n",
        "        networks (list): List of networks\n",
        "    Returns:\n",
        "        float: The average accuracy of a population of networks.\n",
        "    \"\"\"\n",
        "    total_accuracy = 0\n",
        "    for network in networks:\n",
        "        total_accuracy += network.accuracy\n",
        "\n",
        "    return total_accuracy / len(networks)\n",
        "\n",
        "def generate(generations, population, nn_param_choices, dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n):\n",
        "    \"\"\"Generate a network with the genetic algorithm.\n",
        "    Args:\n",
        "        generations (int): Number of times to evole the population\n",
        "        population (int): Number of networks in each generation\n",
        "        nn_param_choices (dict): Parameter choices for networks\n",
        "        dataset (str): Dataset to use for training/evaluating\n",
        "    \"\"\"\n",
        "    optimizer = Optimizer(nn_param_choices)\n",
        "    networks = optimizer.create_population(population)\n",
        "\n",
        "    # Evolve the generation.\n",
        "    for i in range(generations):\n",
        "        print(\"***Doing generation %d of %d***\" %\n",
        "                     (i + 1, generations))\n",
        "\n",
        "        # Train and get accuracy for networks.\n",
        "        train_networks(networks, dataset, train_data,test_data,train_labels,test_labels,dataframe,nh,n)\n",
        "\n",
        "        # Get the average accuracy for this generation.\n",
        "        average_accuracy = get_average_accuracy(networks)\n",
        "\n",
        "        # Print out the average accuracy each generation.\n",
        "        #print('-'*80)\n",
        "        #print(\"Generation average: %.2f%%\" % (average_accuracy * 100))\n",
        "        #print('-'*80)\n",
        "\n",
        "        # Evolve, except on the last iteration.\n",
        "        if i != generations - 1:\n",
        "            # Do the evolution.\n",
        "            networks = optimizer.evolve(networks)\n",
        "\n",
        "    # Sort our final population.\n",
        "    networks = sorted(networks, key=lambda x: x.accuracy, reverse=True)\n",
        "    \n",
        "    return networks[:1]\n",
        "\n",
        "def print_networks(networks):\n",
        "    \"\"\"Print a list of networks.\n",
        "    Args:\n",
        "        networks (list): The population of networks\n",
        "    \"\"\"\n",
        "    print('-'*80)\n",
        "    for network in networks:\n",
        "        network.print_network()\n",
        "        \n",
        "def get_network_train_predictions(networks):\n",
        "  \n",
        "  for network in networks: \n",
        "    its=network.network_train_predictions()\n",
        "  return its\n",
        "\n",
        "def get_network_test_predictions(networks):\n",
        "  \n",
        "  for network in networks: \n",
        "    its=network.network_test_predictions()\n",
        "  return its\n",
        "\n",
        "\n",
        "################################################################################\n",
        "##### CODE FOR MODEL BACKTESTING AND BASIS FOR CUSTOM LOSS FTN #################\n",
        "################################################################################\n",
        "### The main code from this class is based on the Kupiec-Christoffersen 1998   #########\n",
        "### framework which uses a loglikelihood ratio to evaluate the number of exceptions  ###\n",
        "### in financial loss.                                                               ###\n",
        "########################################################################################\n",
        "\n",
        "class Backtest:\n",
        "    def __init__(self, actual, forecast, alpha):\n",
        "        self.index = actual.index\n",
        "        self.actual = actual.values\n",
        "        self.forecast = forecast.values\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def hit_series(self):\n",
        "        return (self.actual < self.forecast) * 1\n",
        "\n",
        "    def number_of_hits(self):\n",
        "        return self.hit_series().sum()\n",
        "\n",
        "    def hit_rate(self):\n",
        "        return self.hit_series().mean()\n",
        "\n",
        "    def expected_hits(self):\n",
        "        return self.actual.size * self.alpha\n",
        "\n",
        "    def duration_series(self):\n",
        "        hit_series = self.hit_series()\n",
        "        hit_series[0] = 1\n",
        "        hit_series[-1] = 1\n",
        "        return np.diff(np.where(hit_series == 1))[0]\n",
        "\n",
        "    def plot(self, file_name=None):\n",
        "\n",
        "        # Re-add the time series index\n",
        "        r = pd.Series(self.actual, index=self.index)\n",
        "        q = pd.Series(self.forecast, index=self.index)\n",
        "\n",
        "        sns.set_context(\"paper\")\n",
        "        sns.set_style(\"whitegrid\", {\"font.family\": \"serif\", \"font.serif\": \"Computer Modern Roman\", \"text.usetex\": True})\n",
        "\n",
        "        # Hits\n",
        "        ax = r[r <= q].plot(color=\"red\", marker=\"o\", ls=\"None\", figsize=(6, 3.5))\n",
        "        for h in r[r <= q].index:\n",
        "            plt.axvline(h, color=\"black\", alpha=0.4, linewidth=1, zorder=0)\n",
        "\n",
        "        # Positive returns\n",
        "        r[q < r].plot(ax=ax, color=\"green\", marker=\"o\", ls=\"None\")\n",
        "\n",
        "        # Negative returns but no hit\n",
        "        r[(q <= r) & (r <= 0)].plot(ax=ax, color=\"orange\", marker=\"o\", ls=\"None\")\n",
        "\n",
        "        # VaR\n",
        "        q.plot(ax=ax, grid=False, color=\"black\", rot=0)\n",
        "\n",
        "        # Axes\n",
        "        plt.xlabel(\"\")\n",
        "        plt.ylabel(\"Log Return\")\n",
        "        ax.yaxis.grid()\n",
        "\n",
        "        sns.despine()\n",
        "        if file_name is None:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.savefig(file_name, bbox_inches=\"tight\")\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    def tick_loss(self, return_mean=True):\n",
        "        loss = (self.alpha - self.hit_series()) * (self.actual - self.forecast)\n",
        "        if return_mean:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def smooth_loss(self, delta=25, return_mean=True):\n",
        "        \"\"\"Gonzalez-Rivera, Lee and Mishra (2004)\"\"\"\n",
        "        loss = ((self.alpha - (1 + np.exp(delta*(self.actual - self.forecast)))**-1) * (self.actual - self.forecast))\n",
        "        if return_mean:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def quadratic_loss(self, return_mean=True):\n",
        "        \"\"\"Lopez (1999); Martens et al. (2009)\"\"\"\n",
        "        loss = (self.hit_series() * (1 + (self.actual - self.forecast)**2))\n",
        "        if return_mean:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def firm_loss(self, c=1, return_mean=True):\n",
        "        \"\"\"Sarma et al. (2003)\"\"\"\n",
        "        loss = (self.hit_series() * (1 + (self.actual - self.forecast)**2) - c*(1-self.hit_series()) * self.forecast)\n",
        "        if return_mean:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def lr_bt(self):\n",
        "        \"\"\"Likelihood ratio framework of Christoffersen (1998)\"\"\"\n",
        "        hits = self.hit_series()   # Hit series\n",
        "        tr = hits[1:] - hits[:-1]  # Sequence to find transitions\n",
        "\n",
        "        # Transitions: nij denotes state i is followed by state j nij times\n",
        "        n01, n10 = (tr == 1).sum(), (tr == -1).sum()\n",
        "        n11, n00 = (hits[1:][tr == 0] == 1).sum(), (hits[1:][tr == 0] == 0).sum()\n",
        "\n",
        "        # Times in the states\n",
        "        n0, n1 = n01 + n00, n10 + n11\n",
        "        n = n0 + n1\n",
        "        print('no '+str(n0))\n",
        "        print('n1 '+str(n1))\n",
        "        print('n '+str(n))\n",
        "\n",
        "        # Probabilities of the transitions from one state to another\n",
        "        p01, p11 = n01 / (n00 + n01), n11 / (n11 + n10)\n",
        "        p = n1 / n\n",
        "        print(str(p))\n",
        "\n",
        "        if n1 > 0:\n",
        "            # Unconditional Coverage\n",
        "            uc_h0 = n0 * np.log(1 - self.alpha) + n1 * np.log(self.alpha)\n",
        "            uc_h1 = n0 * np.log(1 - p) + n1 * np.log(p)\n",
        "            uc = -2 * (uc_h0 - uc_h1)\n",
        "\n",
        "            # Independence\n",
        "            ind_h0 = (n00 + n01) * np.log(1 - p) + (n01 + n11) * np.log(p)\n",
        "            ind_h1 = n00 * np.log(1 - p01) + n01 * np.log(p01) + n10 * np.log(1 - p11)\n",
        "            if p11 > 0:\n",
        "                ind_h1 += n11 * np.log(p11)\n",
        "            ind = -2 * (ind_h0 - ind_h1)\n",
        "\n",
        "            # Conditional coverage\n",
        "            cc = uc + ind\n",
        "\n",
        "            # Stack results\n",
        "            df = pd.concat([pd.Series([uc, ind, cc]),\n",
        "                            pd.Series([1 - stats.chi2.cdf(uc, 1),\n",
        "                                       1 - stats.chi2.cdf(ind, 1),\n",
        "                                       1 - stats.chi2.cdf(cc, 2)])], axis=1)\n",
        "        else:\n",
        "            df = pd.DataFrame(np.zeros((3, 2))).replace(0, np.nan)\n",
        "\n",
        "        # Assign names\n",
        "        df.columns = [\"Statistic\", \"p-value\"]\n",
        "        df.index = [\"Unconditional\", \"Independence\", \"Conditional\"]\n",
        "\n",
        "        return df,p\n",
        "\n",
        "    def dq_bt(self, hit_lags=4, forecast_lags=1):\n",
        "        \"\"\"Dynamic Quantile Test (Engle & Manganelli, 2004)\"\"\"\n",
        "        try:\n",
        "            hits = self.hit_series()\n",
        "            p, q, n = hit_lags, forecast_lags, hits.size\n",
        "            pq = max(p, q - 1)\n",
        "            y = hits[pq:] - self.alpha  # Dependent variable\n",
        "            x = np.zeros((n - pq, 1 + p + q))\n",
        "            x[:, 0] = 1  # Constant\n",
        "\n",
        "            for i in range(p):  # Lagged hits\n",
        "                x[:, 1 + i] = hits[pq-(i+1):-(i+1)]\n",
        "\n",
        "            for j in range(q):  # Actual + lagged VaR forecast\n",
        "                if j > 0:\n",
        "                    x[:, 1 + p + j] = self.forecast[pq-j:-j]\n",
        "                else:\n",
        "                    x[:, 1 + p + j] = self.forecast[pq:]\n",
        "\n",
        "            beta = np.dot(np.linalg.inv(np.dot(x.T, x)), np.dot(x.T, y))\n",
        "            lr_dq = np.dot(beta, np.dot(np.dot(x.T, x), beta)) / (self.alpha * (1-self.alpha))\n",
        "            p_dq = 1 - stats.chi2.cdf(lr_dq, 1+p+q)\n",
        "\n",
        "        except:\n",
        "            lr_dq, p_dq = np.nan, np.nan\n",
        "\n",
        "        return pd.Series([lr_dq, p_dq],\n",
        "                         index=[\"Statistic\", \"p-value\"], name=\"DQ\")\n",
        "      \n",
        "#########################################################################################\n",
        "##### COMPLEXITY MEASURES: ALGORITHMS                          ##########################\n",
        "#########################################################################################\n",
        "### The basic reasoning and set-up of the complexity measures and algorithms        ###\n",
        "### are discussed in the main text.                                                   ###\n",
        "### In summary, fractional dimensions (Katz, Higushi) measure the complexity that     ###\n",
        "### is implied in the stock price by calculting how the power of the time step scales ###\n",
        "### with the 'covered distance'. Hurst exponents measure how the rescaled range of    ###\n",
        "### smaller time step intervals scales with the time step size.                       ### \n",
        "#########################################################################################\n",
        "\n",
        "def katz(data):\n",
        "    n = len(data)-1\n",
        "    L = np.hypot(np.diff(data), 1).sum() # Sum of distances\n",
        "    d = np.hypot(data - data[1], np.arange(len(data))).max() # furthest distance from first point\n",
        "    return log(n) / (log( d/L ) + log( n))\n",
        "\n",
        "def katz2(data):\n",
        "  \n",
        "    first=data.index[0]\n",
        "    dists = np.hypot(np.diff(data), 1)\n",
        "    L = dists.sum()\n",
        "    a = dists.mean()\n",
        "    d = np.hypot(data - first, np.arange(len(data))).max() # furthest distance from first point\n",
        "    return np.log( L/a ) / np.log( d/a)\n",
        "\n",
        "def complexity_fd_higushi(signal, k_max):\n",
        "    \"\"\"\n",
        "    Computes Higuchi Fractal Dimension of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : list or array\n",
        "        List or array of values.\n",
        "    k_max : int\n",
        "        The maximal value of k. The point at which the FD plateaus is considered a saturation point and that kmax value should be selected (Gómez, 2009). Some studies use a value of 8 or 16 for ECG signal and other 48 for MEG.\n",
        "    Returns\n",
        "    ----------\n",
        "    fd_higushi : float\n",
        "        The Higushi Fractal Dimension as float value.\n",
        "    Example\n",
        "    ----------\n",
        "    >>> import neurokit as nk\n",
        "    >>>\n",
        "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
        "    >>> fd_higushi = nk.complexity_fd_higushi(signal, 8)\n",
        "    Notes\n",
        "    ----------\n",
        "    *Details*\n",
        "    - **Higushi Fractal Dimension**: Higuchi proposed in 1988 an efficient algorithm for measuring the FD of discrete time sequences. As the reconstruction of the attractor phase space is not necessary, this algorithm is simpler and faster than D2 and other classical measures derived from chaos theory. FD can be used to quantify the complexity and self-similarity of a signal. HFD has already been used to analyse the complexity of brain recordings and other biological signals.\n",
        "    *Authors*\n",
        "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
        "    *Dependencies*\n",
        "    - numpy\n",
        "    *See Also*\n",
        "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
        "    References\n",
        "    -----------\n",
        "    - Accardo, A., Affinito, M., Carrozzi, M., & Bouquet, F. (1997). Use of the fractal dimension for the analysis of electroencephalographic time series. Biological cybernetics, 77(5), 339-350.\n",
        "    - Gómez, C., Mediavilla, Á., Hornero, R., Abásolo, D., & Fernández, A. (2009). Use of the Higuchi's fractal dimension for the analysis of MEG recordings from Alzheimer's disease patients. Medical engineering & physics, 31(3), 306-313.\n",
        "    \"\"\"\n",
        "    signal = np.array(signal)\n",
        "    L = []\n",
        "    x = []\n",
        "    N = signal.size\n",
        "\n",
        "    km_idxs = np.triu_indices(k_max - 1)\n",
        "    km_idxs = k_max - np.flipud(np.column_stack(km_idxs)) -1\n",
        "    km_idxs[:,1] -= 1\n",
        "\n",
        "\n",
        "    for k in range(1, k_max):\n",
        "        Lk = 0\n",
        "        for m in range(0, k):\n",
        "            #we pregenerate all idxs\n",
        "            idxs = np.arange(1,int(np.floor((N-m)/k)))\n",
        "\n",
        "            Lmk = np.sum(np.abs(signal[m+idxs*k] - signal[m+k*(idxs-1)]))\n",
        "            Lmk = (Lmk*(N - 1)/(((N - m)/ k)* k)) / k\n",
        "            Lk += Lmk\n",
        "\n",
        "        if Lk != 0:\n",
        "            L.append(np.log(Lk/(m+1)))\n",
        "            x.append([np.log(1.0/ k), 1])\n",
        "\n",
        "    (p, r1, r2, s)=np.linalg.lstsq(x, L)\n",
        "    fd_higushi = p[0]\n",
        "    return (fd_higushi)\n",
        "def complexity_entropy_shannon(signal):\n",
        "    \"\"\"\n",
        "    Computes the shannon entropy. Copied from the `pyEntropy <https://github.com/nikdon/pyEntropy>`_ repo by tjugo.\n",
        "    Parameters\n",
        "    ----------\n",
        "    signal : list or array\n",
        "        List or array of values.\n",
        "    Returns\n",
        "    ----------\n",
        "    shannon_entropy : float\n",
        "        The Shannon Entropy as float value.\n",
        "    Example\n",
        "    ----------\n",
        "    >>> import neurokit as nk\n",
        "    >>>\n",
        "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
        "    >>> shannon_entropy = nk.complexity_entropy_shannon(signal)\n",
        "    Notes\n",
        "    ----------\n",
        "    *Details*\n",
        "    - **shannon entropy**: Entropy is a measure of unpredictability of the state, or equivalently, of its average information content.\n",
        "    *Authors*\n",
        "    - tjugo (https://github.com/nikdon)\n",
        "    *Dependencies*\n",
        "    - numpy\n",
        "    *See Also*\n",
        "    - pyEntropy package: https://github.com/nikdon/pyEntropy\n",
        "    References\n",
        "    -----------\n",
        "    - None\n",
        "    \"\"\"\n",
        "     # Check if string\n",
        "    if not isinstance(signal, str):\n",
        "        signal = list(signal)\n",
        "\n",
        "    signal = np.array(signal)\n",
        "\n",
        "\n",
        "    # Create a frequency data\n",
        "    data_set = list(set(signal))\n",
        "    freq_list = []\n",
        "    for entry in data_set:\n",
        "        counter = 0.\n",
        "        for i in signal:\n",
        "            if i == entry:\n",
        "                counter += 1\n",
        "        freq_list.append(float(counter) / len(signal))\n",
        "\n",
        "    # Shannon entropy\n",
        "    shannon_entropy = 0.0\n",
        "    for freq in freq_list:\n",
        "        shannon_entropy += freq * np.log2(freq)\n",
        "    shannon_entropy = -shannon_entropy\n",
        "\n",
        "    return(shannon_entropy)\n",
        "  \n",
        "def bootstrap_resample(X, n=None):\n",
        "    \"\"\" Bootstrap resample an array_like\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "      data to resample\n",
        "    n : int, optional\n",
        "      length of resampled array, equal to len(X) if n==None\n",
        "    Results\n",
        "    -------\n",
        "    returns X_resamples\n",
        "   \n",
        "    if n == None:\n",
        "        n = len(X)\n",
        "        \n",
        "    resample_i = np.floor(np.random.rand(n)*len(X)).astype(int)\n",
        "    X_resample = X[resample_i]\n",
        "    \"\"\"\n",
        "    X_resample = np.random.choice(X, size=n, replace=True)\n",
        "    return X_resample\n",
        "\n",
        "def hurst(ts):\n",
        "  #Returns the Hurst Exponent of the time series vector ts\n",
        "  # Create the range of lag values\n",
        "  lags = range(2, 100)\n",
        "\n",
        "  # Calculate the array of the variances of the lagged differences\n",
        "  tau = [sqrt(std(subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
        "  \n",
        "  # Use a linear fit to estimate the Hurst Exponent\n",
        "  poly = polyfit(log(lags), log(tau), 1)\n",
        "  #plt.scatter(log(lags),log(tau)) \n",
        "  \n",
        "  # Return the Hurst exponent from the polyfit output\n",
        "  return (poly[0]*2.0)\n",
        "\n",
        "################################################################################\n",
        "#### VOLATILITY ESTIMATION            ##########################################\n",
        "################################################################################\n",
        "\n",
        "def vol_est(series, vol_type,n,ticker,error):\n",
        "\n",
        "  returns =((series[close]-series[close].shift(1)) / series[close].shift(1) )\n",
        "  \n",
        "  #OPTIONS: GARCH/EGARCH/ARCH/HARCH normal/t/skewt/ged\n",
        "  g = arch_model(returns, vol=vol_type,p=1,o=0,q=1,dist=error,hold_back=1)\n",
        "  res = g.fit(update_freq=0, disp='off')\n",
        "  #print(res.summary())\n",
        "  ##fig = res.plot(annualize='D')\n",
        "  sigma=res.conditional_volatility\n",
        "  #plt.subplot(211)\n",
        "  #plt.plot(sigma, label = type+\" \"+ticker, color='r')\n",
        "  #_=plt.legend()\n",
        "  #plt.subplot(212)\n",
        "  #plt.plot(returns, label= ticker+\" daily return\",color='g')\n",
        "  #_ = plt.legend()\n",
        "  #plt.show()\n",
        "  return sigma\n",
        "\n",
        "################################################################################\n",
        "### fBM SIMULATION                     #########################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def get_fBM_SIM(h,mean,vol):\n",
        "## Still bullshit: check !!!!\n",
        "  hu=0\n",
        "  if(0>=h):\n",
        "    hu = 0.1\n",
        "  else:\n",
        "    if(h>=1):\n",
        "      hu = 0.8\n",
        "    else: \n",
        "      hu = h\n",
        "    \n",
        "  f = FBM(n=1000, hurst=hu, length=1, method='daviesharte')\n",
        "  # or\n",
        "  #f = FBM(100, 0.5)\n",
        "  # Generate a fBm realization\n",
        "  #fbm_sample = f.fbm()*1000\n",
        "  # Generate a fGn realization\n",
        "  fgn_sample = f.fgn()\n",
        "\n",
        "  # Get the times associated with the fBm\n",
        "  t_values = f.times()\n",
        "\n",
        "  hVaR = -np.min(fgn_sample)\n",
        "  return hVaR\n",
        "\n",
        "################################################################################\n",
        "###  DEFINITIONS LOOKBACK WINDOW N OBS #########################################\n",
        "################################################################################\n",
        "\n",
        "def Mean_n_close(x,n,df):\n",
        "#Mean of last n closing prices\n",
        "  loc = df[close][df[close]==x].index[0]\n",
        "  M = np.mean(df.loc[(loc-n):loc, close])\n",
        "  \n",
        "  return M\n",
        "\n",
        "def hs_n_return(x,n, confidence, df):\n",
        "#Empirical quantiles last n returns\n",
        "  loc = df['d_return'][df['d_return']==x].index[0]\n",
        "  ret = -df.loc[(loc-n):loc, 'd_return']\n",
        "  hs = np.percentile(ret, confidence)\n",
        "  \n",
        "  return hs\n",
        "\n",
        "def simple_fhs_n_return(x,n, confidence, df):\n",
        "#Empirical quantiles last n returns filtered for GARCH vol\n",
        "  loc = df['d_return'][df['d_return']==x].index[0]\n",
        "  vol = df.loc[(loc-n):loc, 'GARCH_vol']\n",
        "  ret = df.loc[(loc-n):loc, 'd_return'] \n",
        "  idd = ret/vol\n",
        "  fhs = np.percentile(idd, confidence)\n",
        "  \n",
        "  return fhs\n",
        "\n",
        "def Mean_n_return(x,n,df):\n",
        "#Mean of last n returns\n",
        "  l = df['d_return'][df['d_return']==x]\n",
        "  loc = df['d_return'][df['d_return']==x].index[0]\n",
        "  M = np.mean(df.loc[(loc-n):loc, 'd_return'])\n",
        "  \n",
        "  return M\n",
        "\n",
        "def Kurt_n_return(x,n,df):\n",
        "#Kurtosis of last n returns\n",
        "  l = df['d_return'][df['d_return']==x]\n",
        "  loc = df['d_return'][df['d_return']==x].index[0]\n",
        "  K = sc.stats.kurtosis(df.loc[(loc-n):loc, 'd_return'])\n",
        "  \n",
        "  return K\n",
        "\n",
        "def Hurst_n_close(x,n,df):\n",
        "#Kurtosis of last n returns\n",
        "  l = df[close][df[close]==x]\n",
        "  loc = df[close][df[close]==x].index[0]\n",
        "  rough_series = df.loc[(loc-n):loc, close]\n",
        "  #geared_rough_series = bootstrap_resample(rough_series,1000) -> don't do this\n",
        "  #h = hurst(geared_rough_series)\n",
        "  h=hurst(rough_series)\n",
        "  return h\n",
        "\n",
        "def true_n_VaR(x,n,cl,df):\n",
        "#Empirical quantiles for 'next' n returns -> TO BE ABANDONNED!!!!\n",
        "  l = df['d_return'][df['d_return']==x]\n",
        "  loc = df['d_return'][df['d_return']==x].index[0]\n",
        "  rough_series = df.loc[loc:(loc+n), 'd_return']\n",
        "  c=np.percentile(rough_series,cl)\n",
        "  return -c\n",
        "  \n",
        "\n",
        "def higushi_n_close(x,n,df):\n",
        "#Higushi FB of last n closing prices chart\n",
        "  l = df[close][df[close]==x]\n",
        "  loc = df[close][df[close]==x].index[0]\n",
        "  rough_series = df.loc[(loc-n):loc, close]\n",
        "  k = 16\n",
        "  c=complexity_fd_higushi(rough_series,k)\n",
        "  return c\n",
        "\n",
        "def shannon_n_close(x,n,df):\n",
        "#Shannon entropy of last n closing prices chart\n",
        "  l = df[close][df[close]==x]\n",
        "  loc = df[close][df[close]==x].index[0]\n",
        "  rough_series = df.loc[(loc-n):loc, close]\n",
        "  c=complexity_entropy_shannon(rough_series)\n",
        "  return c\n",
        "\n",
        "def katz3(x,n,df):\n",
        "#Katz FD of last n closing prices chart\n",
        "  l = df[close][df[close]==x]\n",
        "  loc = df[close][df[close]==x].index[0]\n",
        "  rough_series = df.loc[(loc-n):loc, close]\n",
        "  h=katz2(rough_series)\n",
        "  return h\n",
        "\n",
        "################################################################################\n",
        "### PRESETS DATA SOURCE                   ######################################\n",
        "################################################################################\n",
        "\n",
        "#Pick a data source: datastream (Preferred),'yahoo' (BUG) 'google' (BUG) 'quandl' (COMPLETENESS?) 'morningstar' (BUG) 'iex' (INVESTOR EXCHANGE: ONLY AMERICAN)\n",
        "\n",
        "res = pd.DataFrame(data=None, dtype=np.int8)\n",
        "source = \"datastream\" #@param [\"iex\", \"yahoo\", \"google\", \"morningstar\", \"quandl\", \"datastream\"]\n",
        "close=None\n",
        "\n",
        "if(source == 'morningstar' or source =='google' or source =='quandl'):\n",
        "      close='Close'\n",
        "      volume='Volume'\n",
        "else:\n",
        "      if(source==\"iex\"):\n",
        "        close = 'close'\n",
        "        volume='volume'\n",
        "      else: close = \"Adj Close\"\n",
        "if(source=='datastream'):\n",
        "    close='CLOSE'\n",
        "    volume='VOL'\n",
        "\n",
        "    \n",
        "################################################################################\n",
        "### MAIN FUNCTION: ANALYSIS IN 'DO'                    #########################\n",
        "################################################################################\n",
        "\n",
        "def do(t,count, nr_of_generations, population_size):  \n",
        "  \n",
        "# Trains a neural net based on different VaRs and fractal complexity measures of\n",
        "# stock with ticker t, over 'nr_of_generations' generations with an initial population\n",
        "# size of 'population_size'. Returns a result dataframe res that is subsequently absorbed\n",
        "# by a summary dataframe summ that is filled with the different stocks over time and their backtest results.\n",
        "  \n",
        "  ticker = t \n",
        "\n",
        "  starting_date = \"2004-03-01\" #@param {type:\"date\"}\n",
        "  ending_date = \"2019-03-01\" #@param {type:\"date\"}\n",
        "  \n",
        "  if(source=='datastream'):\n",
        "    df=DWE.get_OHLCV(ticker,date_from=starting_date,date_to=ending_date)\n",
        "    #df = DWE.fetch(ticker, ['PI','VO'], date_from=starting_date,date_to=ending_date)\n",
        "    df = df.reset_index(drop=True)\n",
        "  lookback_period = 500 #@param {type:\"integer\"}\n",
        "  confidence_level = 0.99 #@param {type:\"number\"}\n",
        "  hurst_n = 500 #@param {type:\"number\"}\n",
        "  nh = hurst_n\n",
        "  lb = lookback_period\n",
        "  cl = confidence_level\n",
        " \n",
        "  \"\"\"\n",
        "  df2=pd.DataFrame(data=None)\n",
        "  df2['CLOSE']=df['PI']\n",
        "  df2['VOL']=df['VO']\n",
        "  df=df2\n",
        "  \"\"\"\n",
        "  \n",
        "  print(ticker)\n",
        "  print(source)\n",
        "  print(starting_date)\n",
        "  print(ending_date)\n",
        "  \n",
        "  data_source=source\n",
        "  start_date = starting_date\n",
        "  end_date = ending_date\n",
        "  # Download\n",
        "\n",
        "  if(source!='datastream'):\n",
        "    df = data.DataReader(ticker , data_source, start_date, end_date)\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "  #df.dropna(inplace=True)\n",
        "  df.fillna(df.mean(), inplace=True)\n",
        "  \n",
        "  #Calculate daily logreturns\n",
        "  df['d_return'] = log(df[close] / df[close].shift(1))\n",
        "\n",
        "  plt.plot(df[close], label = ticker+\" quote \"+starting_date+\" - \"+ending_date)\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  ### Estimation of daily GARCH volatility with following options:\n",
        "  ###    * GARCH/EGARCH/ARCH/HARCH  with normal/t/skewt/ged errors -->  !!! EVALUATE !!! \n",
        "  \n",
        "  df['GARCH_vol'] = vol_est(df, 'GARCH',412,ticker,'normal')\n",
        "  #df['EGARCH_vol'] = vol_est(df, 'EGARCH',413,ticker,'normal')\n",
        "  #df['HARCH_vol'] = vol_est(df, 'HARCH',414, ticker,'normal')\n",
        "  #df['GARCH_vol'] = vol_est(df, 'GARCH',412,ticker,'t')\n",
        "  #df['ARCH_vol'] = vol_est(df, 'EGARCH',413,ticker,'t')\n",
        "  #df['EGARCH_vol'] = vol_est(df, 'ARCH',413,ticker,'t')\n",
        "  #df['HARCH_vol'] = vol_est(df, 'HARCH',414, ticker,'t')\n",
        "  #df['GARCH_vol'] = vol_est(df, 'GARCH',412,ticker,'skewt')\n",
        "  #df['ARCH_vol'] = vol_est(df, 'EGARCH',413,ticker,'skewt')\n",
        "  #df['EGARCH_vol'] = vol_est(df, 'HGARCH',413,ticker,'skewt')\n",
        "  #df['HARCH_vol'] = vol_est(df, 'ARCH',414, ticker,'skewt')\n",
        "  #df['GARCH_vol'] = vol_est(df, 'GARCH',412,ticker,'ged')\n",
        "  #df['ARCH_vol'] = vol_est(df, 'EGARCH',413,ticker,'ged')\n",
        "  #df['EGARCH_vol'] = vol_est(df, 'HGARCH',413,ticker,'ged')\n",
        "  #df['HARCH_vol'] = vol_est(df, 'ARCH',414, ticker,'ged')\n",
        "\n",
        "  df[\"Mean_\"+str(lb)] = 0.00\n",
        "  for i in range(1,len(df['d_return'])):\n",
        "    df[\"Mean_\"+str(lb)][i]=Mean_n_return(df['d_return'][i],lb,df)\n",
        "\n",
        "  df[\"Empirical_VaR_\"+str(lb)] = 0.00\n",
        "  for i in range(1,len(df['d_return'])):\n",
        "    df[\"Empirical_VaR_\"+str(lb)][i]=true_n_VaR(df['d_return'][i],lb/2,(1-cl),df)\n",
        "\n",
        "\n",
        "  df[\"Hurst_\"+str(nh)] = 0.00\n",
        "  for i in range((nh-1),len(df[close])):\n",
        "    df[\"Hurst_\"+str(nh)][i]=Hurst_n_close(df[close][i],nh,df)\n",
        "\n",
        "  #df['fBM_SIM']= 0\n",
        "  #for i in range(0,len(df[close])):\n",
        "  #  df['fBM_SIM'][i] = get_fBM_SIM(df[\"Hurst_\"+str(nh)][i], df[\"Mean_\"+str(lb)][i],df['GARCH_vol'][i])\n",
        "  \n",
        "  df[\"Higushi_FD_\"+str(nh)]=1.5\n",
        "  for i in range((nh-1), len(df[close])):\n",
        "    df['Higushi_FD_'+str(nh)][i]= higushi_n_close(df[close][i],nh,df)\n",
        "  \n",
        "  #df[\"Shannon_\"+str(nh)]=5\n",
        "  #for i in range((nh-1), len(df[close])):\n",
        "  #  df['Shannon_'+str(nh)][i]= shannon_n_close(df[close][i],nh,df)\n",
        "\n",
        "  df[\"Katz_\"+str(nh)]=1.5\n",
        "  for i in range((nh-1), len(df[close])):\n",
        "    df['Katz_'+str(nh)][i]= katz3(df[close][i],nh,df)\n",
        "\n",
        "  df['NVaR']=  - df[\"Mean_\"+str(lb)] + df['GARCH_vol']*(norm.ppf(cl))\n",
        "  \n",
        "  #NES(n,1) = port*(-mean(returns)+(normpdf(norminv(cl))/(1-cl))*std(returns))\n",
        "  #v = (6+4*Kurt_n_return(df['d_return'],lb))/(Kurt_n_return(df['d_return'],lb)) \n",
        "  #TVAR(n,1) = port*(-mean(returns) + tinv(cl,v) * sqrt((v-2)/v) * std(returns) )\n",
        "  #LVAR(n,1) = port* ( 1-exp( mean(returns) - std(returns)*norminv(cl) ) )\n",
        "\n",
        "  df['NES']= - df[\"Mean_\"+str(lb)] + df['GARCH_vol']*(norm.pdf((norm.ppf(cl)))/(1-cl))\n",
        "\n",
        "  df['TVaR']=0\n",
        "  for i in range(1,len(df[close])):\n",
        "    v = (6+4*Kurt_n_return(df['d_return'][i],nh,df))/(Kurt_n_return(df['d_return'][i],nh,df))\n",
        "    df['TVaR'][i]= - df[\"Mean_\"+str(lb)][i] + sc.stats.t.ppf(cl,v) * np.sqrt((v-2)/v) * df['GARCH_vol'][i]\n",
        "\n",
        "  df['logVaR']= (1-np.exp(df[\"Mean_\"+str(lb)] - df['GARCH_vol']*(norm.ppf(cl))))\n",
        "  \n",
        "  df['HS']=0\n",
        "  for i in range(1,len(df[close])):\n",
        "    df['HS'][i]= hs_n_return(df['d_return'][i],nh,cl,df)\n",
        "  df['HS']=-df['HS']\n",
        "  \n",
        "  df['FHS']=0\n",
        "  for i in range(1,len(df[close])):\n",
        "    df['FHS'][i]= simple_fhs_n_return(df['d_return'][i],nh,cl,df)*df['GARCH_vol'][i]\n",
        "  df['FHS']=-df['FHS']\n",
        "  \n",
        "  eta = 0.2\n",
        "  df['FVaR']=  df[\"Mean_\"+str(lb)] - ( df['GARCH_vol'] / eta )*(1-np.power(-np.log(cl),-eta))\n",
        "  df['GVaR']=  df[\"Mean_\"+str(lb)] - df['GARCH_vol']*np.log(-np.log(cl))\n",
        "  \n",
        "  df.fillna(df.mean(), inplace=True)\n",
        "  \n",
        "  display.display(df.describe())\n",
        "  display.display(df.head())\n",
        "  display.display(df.tail())\n",
        "\n",
        "\n",
        "  o=np.mean(df['Hurst_'+str(nh)][nh:])\n",
        "\n",
        "  if(count==0):\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df['d_return'][nh:], label='return')\n",
        "    _=plt.legend()\n",
        "    plt.show()\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df[\"Hurst_\"+str(nh)][nh:],label='Hurst',color='y',marker='|')\n",
        "    k=pd.Series(len(df['Hurst_'+str(nh)]))\n",
        "    for i in range(0,len(df['Hurst_'+str(nh)])):\n",
        "      k[i]=o\n",
        "    plt.plot(k[nh:], label='Mean Hurst')\n",
        "    _=plt.legend()\n",
        "    plt.show()\n",
        "    #plt.subplot(111)\n",
        "    #plt.plot(df['Shannon_'+str(nh)],label=ticker)\n",
        "    #_=plt.legend()\n",
        "    #plt.show()\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df[close][nh:],label=ticker)\n",
        "    _=plt.legend()\n",
        "    plt.show()\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df['GARCH_vol'][nh:], label='GARCH Vol', color='r')\n",
        "    _=plt.legend()\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df['NVaR'][nh:], label = 'NVaR')\n",
        "    plt.plot(df['TVaR'][nh:],label=\"TVaR\", c='c')\n",
        "    plt.plot(df['FVaR'][nh:],label=\"FVaR\", c='b')\n",
        "    _=plt.legend()\n",
        "    plt.show()\n",
        "    plt.subplot(111)\n",
        "    plt.plot(df['GVaR'][nh:],label=\"GVaR\", c='g')\n",
        "    plt.plot(df['FHS'][nh:],label=\"FHS\", c='r')\n",
        "    plt.plot(df['HS'][nh:],label=\"HS\", c='y')\n",
        "    _=plt.legend()\n",
        "    plt.show()\n",
        "  #fig = plt.figure()\n",
        "  #ax1 = fig.add_subplot(111, projection='3d')\n",
        "  #Axes3D.plot_trisurf(ax1,df['Hurst_'+str(nh)][nh:], df['GARCH_vol'][nh:],df['NVaR'][nh:])\n",
        "  #Axes3D.scatter(ax1,df['Hurst_'+str(nh)][nh:],df['GARCH_vol'][nh:],df['logVaR'][nh:],cmap='seismic', c=df['logVaR'][nh:].shift(-1) / np.max(df['NVaR'][nh:]))\n",
        "\n",
        "  \n",
        "  ##############################################################################\n",
        "  ### START OF MACHINE LEARNING: PREPROCESSING                 #################\n",
        "  ##############################################################################\n",
        "  \n",
        "  def preprocess_features(df):\n",
        "    selected_features =df[\n",
        "      [\n",
        "       volume,\n",
        "       \"HS\",\n",
        "       \"FHS\",\n",
        "       \"FVaR\",\n",
        "       \"GVaR\",\n",
        "       #\"Katz_\"+str(nh),\n",
        "       \"Hurst_\"+str(nh),\n",
        "       \"GARCH_vol\",\n",
        "       \"Higushi_FD_\"+str(nh),\n",
        "       #\"Shannon_\"+str(nh),\n",
        "       \"NVaR\",\n",
        "       \"TVaR\",\n",
        "       \"logVaR\" \n",
        "      ]]\n",
        "\n",
        "    processed_features = selected_features.copy()\n",
        "    # Create a synthetic feature?\n",
        "\n",
        "    processed_features[volume] = df[volume]/(df[volume].max())\n",
        "\n",
        "\n",
        "    #processed_features[\"Katz_\"+str(nh)] = df[\"Katz_\"+str(nh)]/(2)\n",
        "    #processed_features[\"Higushi_FD_\"+str(nh)] = df[\"Higushi_FD_\"+str(nh)]/(2)\n",
        "    #processed_features[\"Shannon_\"+str(nh)] = df[\"Shannon_\"+str(nh)]/(df[\"Shannon_\"+str(nh)].max())\n",
        "    return processed_features\n",
        "    \n",
        "  def preprocess_features_wo_aor(df):\n",
        "    selected_features =df[\n",
        "      [\n",
        "       \"HS\",\n",
        "       \"FHS\",\n",
        "       \"FVaR\",\n",
        "       \"GVaR\",\n",
        "       \"NVaR\",\n",
        "       \"TVaR\",\n",
        "       \"logVaR\" \n",
        "      ]]\n",
        "\n",
        "    processed_features = selected_features.copy()\n",
        "    # Create a synthetic feature?\n",
        "\n",
        "    #processed_features[volume] = df[volume]/(df[volume].max())\n",
        "\n",
        "\n",
        "    #processed_features[\"Katz_\"+str(nh)] = df[\"Katz_\"+str(nh)]/(2)\n",
        "    #processed_features[\"Higushi_FD_\"+str(nh)] = df[\"Higushi_FD_\"+str(nh)]/(2)\n",
        "    #processed_features[\"Shannon_\"+str(nh)] = df[\"Shannon_\"+str(nh)]/(df[\"Shannon_\"+str(nh)].max())\n",
        "\n",
        "\n",
        "    return processed_features\n",
        "  \n",
        "\n",
        "  def preprocess_targets(df):\n",
        "\n",
        "    output_targets = pd.DataFrame()\n",
        "    output_targets[\"Empirical_VaR_\"+str(lb)] = df['Empirical_VaR_'+str(lb)]\n",
        "    \n",
        "    return output_targets\n",
        "\n",
        "  # Choose the first examples for training.\n",
        "\n",
        "  ### !!!! Reconsider this test vs validation sample distinction !!!!\n",
        "  \n",
        "  q=max(200,round(len(df[close])/5))\n",
        "  n = len(df[close])-q\n",
        "  n = len(df[close])-q\n",
        "  training_examples = preprocess_features(df[nh:n])\n",
        "  training_targets = preprocess_targets(df[nh:n])\n",
        "  train_data_wo_aor = preprocess_features_wo_aor(df[nh:n])\n",
        "\n",
        "  # Choose the last q examples for validation.\n",
        "  validation_examples = preprocess_features(df.tail(q))\n",
        "  validation_targets = preprocess_targets(df.tail(q))\n",
        "  test_data_wo_aor = preprocess_features_wo_aor(df.tail(q))\n",
        "  \n",
        "  validation_returns = df[\"d_return\"].tail(q)\n",
        "  test_nvar = df['NVaR'].tail(q)\n",
        "  test_tvar = df['TVaR'].tail(q)\n",
        "  test_logvar = df['logVaR'].tail(q)\n",
        "  test_fvar = df['FVaR'].tail(q)\n",
        "  test_FHS = df['FHS'].tail(q)\n",
        "  test_gvar = df['GVaR'].tail(q)\n",
        "  test_HS = df['HS'].tail(q)\n",
        "  \"\"\"\n",
        "  # Double-check that we've done the right thing.\n",
        "  if(count==0):\n",
        "    print(\"Training examples summary:\")\n",
        "    display.display(training_examples.describe())\n",
        "    print(\"Validation examples summary:\")\n",
        "    display.display(validation_examples.describe())\n",
        "\n",
        "    print(\"Training targets summary:\")\n",
        "    display.display(training_targets.describe())\n",
        "    print(\"Validation targets summary:\")\n",
        "    display.display(validation_targets.describe())\n",
        "    \n",
        "    plt.subplot(111)\n",
        "    plt.plot(validation_targets, label= ticker+\" NY plot\",color='g')\n",
        "    _ = plt.legend()\n",
        "    plt.show()\n",
        "    \"\"\" \n",
        "  \n",
        "  def construct_feature_columns(input_features):\n",
        "    return set([tf.feature_column.numeric_column(my_feature)\n",
        "                for my_feature in input_features])\n",
        "  \n",
        "  def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
        "\n",
        "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
        "\n",
        "   # Construct a dataset, and configure batching/repeating.\n",
        "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "  # Shuffle the data, if specified.\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)\n",
        "\n",
        "  # Return the next batch of data.\n",
        "    features, labels = ds.make_one_shot_iterator().get_next()\n",
        "    return features, labels\n",
        "  \n",
        "  train_data=training_examples \n",
        "  train_labels=training_targets\n",
        "  test_data=validation_examples\n",
        "  test_labels=validation_targets\n",
        "\n",
        "  # Shuffle the training set  --> dubble check this\n",
        "  # order = np.argsort(np.random.random(train_labels.shape))\n",
        "  # train_data = train_data[order]\n",
        "  # train_labels = train_labels[order]\n",
        "\n",
        "  print(\"Training set: {}\".format(train_data.shape)) \n",
        "  print(\"Testing set:  {}\".format(test_data.shape)) \n",
        "\n",
        "  print(train_labels[0:10])\n",
        "  \n",
        "  ##############################################################################\n",
        "  ### LET'S BUILD OUR NEURAL NETWORK                           #################\n",
        "  ##############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "  #### Comment/Uncomment next section not to/to have genetic algorithms: meant for development purposes\n",
        "  #\"\"\"\n",
        "  def build_model(train_data_to_build): \n",
        "    model = keras.Sequential([\n",
        "      keras.layers.Dense(100, activation=tf.nn.relu,\n",
        "                         input_shape=(train_data_to_build.shape[1],)),\n",
        "      keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "      keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "      keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    myoptimizer = tf.train.AdamOptimizer(0.0001)\n",
        "\n",
        "    \"\"\"\n",
        "    def myloss(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      c = K.cast(K.greater(returntensor,y_pred), 'float32')\n",
        "      #c=tf.reduce_sum(errorGreater)\n",
        "      total = K.cast(K.greater(y_pred,-1000),'float32')\n",
        "      #total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      customloss=customloss+penalty*(1-cl-p)*(1-cl-p)\n",
        "      return customloss\n",
        "    \"\"\"\n",
        "    \n",
        "    def myloss(y_true,y_pred):\n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      #errorGreater = K.cast(K.greater(y_pred ,returntensor), 'float32')\n",
        "      errorGreater = K.cast(K.greater(y_pred,returntensor), 'float32')\n",
        "      c=tf.reduce_sum(errorGreater)\n",
        "      errorGreater2 = K.cast(K.greater(y_pred,-10000),'float32')\n",
        "      total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      #customloss=10*(cl-p)*(cl-p)+customloss\n",
        "      #customloss=penalty*(cl-p)*(cl-p)+customloss\n",
        "      customloss=penalty*(cl-(1-p))*(cl-(1-p))+customloss\n",
        "      return customloss\n",
        "    \n",
        "    def metric_p(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      c = K.cast(K.greater(returntensor,y_pred), 'float32')\n",
        "      #c=tf.reduce_sum(errorGreater)\n",
        "      total = K.cast(K.greater(y_pred,-1000),'float32')\n",
        "      #total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      \n",
        "      return p\n",
        "    \n",
        "    def metric_dev(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      c = K.cast(K.greater(y_pred,returntensor), 'float32')\n",
        "      #c=tf.reduce_sum(errorGreater)\n",
        "      total = K.cast(K.greater(y_pred,-10000),'float32')\n",
        "      #total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total #NO PARSING\n",
        "      diff = 1-p\n",
        "      n0 = (total[0,0]-c[0,0])*len(df['d_return'][nh:])\n",
        "      n1 = c[0,0]*len(df['d_return'][nh:])\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      #unitytf=tf.constant(1.00000, dtype='float32',shape=c[0].get_shape())\n",
        "      #diff = tf.math.subtract(unitytf,c[0])\n",
        "      #with sess.as_default():\n",
        "       # def takelog(x):\n",
        "        #  return tf.log(x)\n",
        "      #y=sess.run(diff)\n",
        "      \n",
        "      #diff=y[0]\n",
        "      #logdiff = np.log(y[0])\n",
        "      \n",
        "      log1 = 0\n",
        "      log2 = K.log(c[0])\n",
        "      \n",
        "      uc_h1 = n0 * log1 + n1 * log2\n",
        "      #uc_h1 = n0 * K.log(diff[0]) + n1 * K.log(c[0,0])\n",
        "      #uc_h1 = n0 * K.log(1 - c[0,0]) + n1 * K.log(c[0,0])\n",
        "      #uc_h1 =K.log(1-c[0,0])\n",
        "      #uc_h1 = n0 * np.log(1 - p) + n1 * np.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      \n",
        "      #return cl-p\n",
        "      return (1-cl-p)\n",
        " \n",
        "    def metric_total(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      c= K.cast(K.greater(y_pred,returntensor), 'float32')\n",
        "      #c=tf.reduce_sum(errorGreater)\n",
        "      total = K.cast(K.greater(y_pred,-10000),'float32')\n",
        "      #total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      \n",
        "      return total\n",
        "  \n",
        "    def metric_c(y_true,y_pred):\n",
        "      \n",
        "      \n",
        "      customloss=keras.losses.mean_squared_error(y_true, y_pred)\n",
        "\n",
        "      #INTRODUCE KUPIEC\n",
        "      #int_y1 = K.cast([100*x for x in y_pred],'int32')\n",
        "      \n",
        "      returntensor = tf.convert_to_tensor(-df['d_return'][nh:])\n",
        "      #int_y2 = K.cast([100*x for x in df['d_return']],'int32')\n",
        "      #int_y2 = K.cast([x for x in returntensor],'int32')\n",
        "      \n",
        "      y_pred = K.cast(y_pred, 'float32')\n",
        "      returntensor = K.cast(returntensor, 'float32')\n",
        "     \n",
        "      c = K.cast(K.greater(y_pred,returntensor), 'float32')\n",
        "      #c=tf.reduce_sum(errorGreater)\n",
        "      total = K.cast(K.greater(y_pred,-10000),'float32')\n",
        "      #total=tf.reduce_sum(errorGreater2)\n",
        "      \n",
        "      alpha = 1-cl\n",
        "      p = c/total\n",
        "      \n",
        "      n0 = total-c\n",
        "      n1 = c\n",
        "\n",
        "      uc_h0 = n0 * np.log(1 - alpha) + n1 * np.log(alpha)\n",
        "      uc_h1 = n0 * K.log(1 - p) + n1 * K.log(p)\n",
        "      uc = -2 * (uc_h0 - uc_h1)\n",
        "      #chi = tfp.distributions.Chi2(1, validate_args=False, allow_nan_stats=True, name='Chi2')\n",
        "      chi = tf.distributions.Gamma(concentration=0.5 , rate=0.5, validate_args=False, allow_nan_stats=True, name='Gamma')\n",
        "      #chi = tf.contrib.distributions.chi2(1)\n",
        "      prob = 1-chi.cdf(uc, name= 'cdf')\n",
        "      \n",
        "      \n",
        "      #customloss = p  #NO GRADIENTS AVAILABLE\n",
        "      \n",
        "      #customloss = keras.losses.mean_squared_error(y_true, y_pred) + p #'SOME' FAIL TO DIFFERENTIATE\n",
        "      #k=5\n",
        "      #customloss = k*(1-prob)+customloss\n",
        "      \n",
        "      return c\n",
        "\n",
        "    model.compile(loss=myloss, \n",
        "                  optimizer=myoptimizer,\n",
        "                  metrics=[metric_p, metric_dev, metric_total,metric_c,'mae'])\n",
        "    return model\n",
        "\n",
        "  model_rough = build_model(train_data)\n",
        "  model_rough.summary()\n",
        "  model_wo_aor = build_model(train_data_wo_aor)\n",
        "  model_wo_aor.summary()\n",
        "\n",
        "  # Display training progress by printing a single dot for each completed epoch\n",
        "  class PrintDot(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "      if epoch % 1000 == 0: print('.',end=' ')\n",
        "\n",
        "  EPOCHS = 100\n",
        "  print('Progress:       (.=10%)     ')\n",
        "  # Store training stats\n",
        "  history_rough = model_rough.fit(train_data, train_labels, epochs=EPOCHS,\n",
        "                      validation_split=0.2, verbose=0,\n",
        "                      callbacks=[PrintDot()])\n",
        "  history_wo_aor = model_wo_aor.fit(train_data_wo_aor, train_labels, epochs=EPOCHS,\n",
        "                      validation_split=0.2, verbose=0,\n",
        "                      callbacks=[PrintDot()])\n",
        "  \n",
        "  def plot_history(history):\n",
        "    plt.subplot(111)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(history.epoch, np.array(history.history['loss']),\n",
        "             label='Train Loss',color='r')\n",
        "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
        "             label = 'Val loss',color='g')\n",
        "    plt.legend()\n",
        "    #plt.ylim([0,0.40])\n",
        "    plt.show()\n",
        "\n",
        "  # The patience parameter is the amount of epochs to check for improvement\n",
        "  #early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "  \n",
        "  #history = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
        "   #                   validation_split=0.2, verbose=0,\n",
        "    #                  callbacks=[early_stopper, PrintDot()])\n",
        "\n",
        "  plot_history(history_rough)\n",
        "  plot_history(history_wo_aor)\n",
        "  \n",
        "  [loss, metric_p, metric_dev, metric_total,metric_c, mae] = model_rough.evaluate(test_data, test_labels, verbose=1)\n",
        "  [loss2, metric_p2, metric_dev2, metric_total2,metric_c2, mae2] = model_wo_aor.evaluate(test_data_wo_aor, test_labels, verbose=1)\n",
        "  \n",
        "\n",
        "  print(\"Testing set prob loss: {:7.7f}\".format(metric_dev))\n",
        "  print(\"Testing set mae loss: {:7.7f}\".format(mae))\n",
        "  \n",
        "  print(\"Testing set prob loss (wo roughness): {:7.7f}\".format(metric_dev2))\n",
        "  print(\"Testing set mae loss (wo roughness): {:7.7f}\".format(mae2))\n",
        "  \n",
        "  res = pd.DataFrame(data=None, dtype=np.int8)\n",
        "  res['Return']=None\n",
        "  res['Return']=df['d_return'][nh:n]\n",
        "  \n",
        "  ### Insert Kupiec Test Set\n",
        "  res_test = pd.DataFrame(data=None, dtype=np.int8)\n",
        "  res_test['returns']=validation_returns\n",
        "  res_test['test_predictions']=None\n",
        "  res_test['test_predictions_wo_aor']=None\n",
        "  res_test['test_nvar']=None\n",
        "  res_test['test_gvar']=None\n",
        "  res_test['test_fvar']=None\n",
        "  res_test['test_FHS']=None\n",
        "  res_test['test_logvar']=None\n",
        "  res_test['test_HS']=None\n",
        "  \n",
        "  test_predictions = model_rough.predict(test_data)\n",
        "  train_predictions = model_rough.predict(train_data)\n",
        "  test_predictions_wo_aor=model_wo_aor.predict(test_data_wo_aor)\n",
        "  train_predictions_wo_aor=model_wo_aor.predict(train_data_wo_aor)\n",
        "  res_test['test_predictions']=test_predictions\n",
        "  res_test['test_predictions_wo_aor']=test_predictions_wo_aor\n",
        "  #res_test['returns']=validation_returns\n",
        "  res_test['test_nvar']=test_nvar\n",
        "  res_test['test_gvar']=test_gvar\n",
        "  res_test['test_fvar']=test_fvar\n",
        "  res_test['test_FHS']=test_FHS\n",
        "  res_test['test_logvar']=test_logvar\n",
        "  res_test['test_HS']=test_HS\n",
        "  \n",
        "  \n",
        "  print(\"Backtest Rough Model out-of-sample \"+ticker)\n",
        "  bt_test_m=Backtest(res_test['test_predictions'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_m.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_m.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_m.dq_bt())\n",
        "  kupiec_c_model_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_model_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_model_oos=kupiecp\n",
        "  #bt_test_m.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: Model\")\n",
        "  #print(bt_test_m.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Model\")\n",
        "  #print(bt_test_m.dq_bt())\n",
        "  \n",
        "  print(\"Backtest Combination Model out-of-sample \"+ticker)\n",
        "  bt_test_m_wo_aor=Backtest(res_test['test_predictions_wo_aor'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_m_wo_aor.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_m_wo_aor.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_m_wo_aor.dq_bt())\n",
        "  kupiec_c_model_wo_aor_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_model_wo_aor_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_model_wo_aor_oos=kupiecp\n",
        "  #bt_test_m.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: Model\")\n",
        "  #print(bt_test_m.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Model\")\n",
        "  #print(bt_test_m.dq_bt())\n",
        "  \n",
        "  print(\"Backtest Normal out-of-sample \"+ticker)\n",
        "  bt_test_n=Backtest(res_test['test_nvar'],res_test['returns'],(1-cl))\n",
        "  #bt_test_n.plot()\n",
        "  \n",
        "  [kupiec,kupiecp]=bt_test_n.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_n.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_n.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_n.dq_bt())\n",
        "  kupiec_c_normal_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_normal_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_normal_oos=kupiecp\n",
        "    \n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: Normal\")\n",
        "  #print(bt_test_n.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Normal\")\n",
        "  #print(bt_test_n.dq_bt())\n",
        "  \n",
        "  \n",
        "  print(\"Backtest Gumbel out-of-sample \"+ticker)\n",
        "  bt_test_g=Backtest(res_test['test_gvar'],res_test['returns'],(1-cl))\n",
        "  #bt_test_g.plot()\n",
        "  \n",
        "  [kupiec,kupiecp]=bt_test_g.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_g.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_g.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_g.dq_bt())\n",
        "  kupiec_c_gumbel_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_gumbel_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_gumbel_oos=kupiecp\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: Gumbel\")\n",
        "  #print(bt_test_g.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Gumbel\")\n",
        "  #print(bt_test_g.dq_bt())\n",
        "  \n",
        "  print(\"Backtest HS out-of-sample \"+ticker)\n",
        "  bt_test_hs=Backtest(res_test['test_HS'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_hs.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_hs.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_hs.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_hs.dq_bt())\n",
        "  kupiec_c_hs_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_hs_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_hs_oos=kupiecp\n",
        "  #bt_test_g.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: HS\")\n",
        "  #print(bt_test_hs.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: HS\")\n",
        "  #print(bt_test_hs.dq_bt())\n",
        "  \n",
        "  print(\"Backtest lognormal out-of-sample \"+ticker)\n",
        "  bt_test_logvar=Backtest(res_test['test_logvar'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_logvar.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_hs.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_logvar.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_logvar.dq_bt())\n",
        "  kupiec_c_logvar_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_logvar_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_logvar_oos=kupiecp\n",
        "  #bt_test_g.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: HS\")\n",
        "  #print(bt_test_hs.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: HS\")\n",
        "  #print(bt_test_hs.dq_bt())\n",
        "  \n",
        "  print(\"Backtest FHS out-of-sample \"+ticker)\n",
        "  bt_test_fhs=Backtest(res_test['test_FHS'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_fhs.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_hs.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_fhs.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_fhs.dq_bt())\n",
        "  kupiec_c_fhs_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_fhs_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_fhs_oos=kupiecp\n",
        "  #bt_test_g.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: HS\")\n",
        "  #print(bt_test_hs.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: HS\")\n",
        "  #print(bt_test_hs.dq_bt())\n",
        "  \n",
        "  print(\"Backtest FVaR out-of-sample \"+ticker)\n",
        "  bt_test_fvar=Backtest(res_test['test_fvar'],res_test['returns'],(1-cl))\n",
        "  [kupiec,kupiecp]=bt_test_fvar.lr_bt()\n",
        "  #if(kupiecp!=0):\n",
        "    #bt_test_hs.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_test_fvar.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_test_fvar.dq_bt())\n",
        "  kupiec_c_fvar_oos=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_fvar_oos=kupiec[\"p-value\"][0]\n",
        "  kupiecp_fvar_oos=kupiecp\n",
        "  #bt_test_g.plot()\n",
        "  #print(\"Kupiec-Christoffersen loglikelihood on test set: HS\")\n",
        "  #print(bt_test_hs.lr_bt())\n",
        "  #print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: HS\")\n",
        "  #print(bt_test_hs.dq_bt())\n",
        "   \n",
        "  \n",
        "  #predictions = model.predict(train_data)\n",
        "  \n",
        "  #\"\"\"\n",
        "  #####\n",
        "  \n",
        "  ### GENETIC ALGOS ### \n",
        "  ### ON / OFF ###\n",
        "  \"\"\"\n",
        "  generations = nr_of_generations  # Number of times to evole the population.\n",
        "  population = population_size  # Number of networks in each generation.\n",
        "  dataset = 'thesis'\n",
        "\n",
        "  nn_param_choices = {\n",
        "        'nb_neurons': [2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
        "        'nb_layers': [1, 2, 3, 4],\n",
        "        'activation': ['relu', 'elu', 'tanh', 'sigmoid'],\n",
        "        'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n",
        "                      'adadelta', 'adamax', 'nadam'],\n",
        "    }\n",
        "\n",
        "  print(\"***Evolving %d generations with population %d***\" %\n",
        "                 (generations, population))\n",
        "  print('                                                   ')\n",
        "  print('                                                   ')\n",
        "  \n",
        "  train_data=training_examples \n",
        "  train_labels=training_targets\n",
        "  test_data=validation_examples\n",
        "  test_labels=validation_targets\n",
        "  result = generate(generations, population, nn_param_choices, dataset, train_data ,test_data,train_labels,test_labels,df,nh,n)\n",
        "  #include predictions\n",
        "  \n",
        "  print_networks(result)\n",
        "  test_predictions = get_network_test_predictions(result)\n",
        "  train_predictions = get_network_train_predictions(result)\n",
        "  \n",
        "  res = pd.DataFrame(data=None, dtype=np.int8)\n",
        "  res['Return']=None\n",
        "  res['Return']=df['d_return'][nh:n]\n",
        "  \n",
        "  ### Insert Kupiec Test Set\n",
        "  res_test = pd.DataFrame(data=None, dtype=np.int8)\n",
        "  res_test['returns']=validation_returns\n",
        "  res_test['test_predictions']=None\n",
        "  res_test['test_nvar']=None\n",
        "  res_test['test_gvar']=None\n",
        "  res_test['test_HS']=None\n",
        "  \n",
        "  #test_predictions = model.predict(test_data)\n",
        "  res_test['test_predictions']=test_predictions\n",
        "  #res_test['returns']=validation_returns\n",
        "  res_test['test_nvar']=test_nvar\n",
        "  res_test['test_gvar']=test_gvar\n",
        "  res_test['test_HS']=test_HS\n",
        "  bt_test_m=Backtest(res_test['test_predictions'],res_test['returns'],(1-cl))\n",
        "  #bt_test_m.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood on test set: Model\")\n",
        "  print(bt_test_m.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Model\")\n",
        "  print(bt_test_m.dq_bt())\n",
        "  bt_test_n=Backtest(res_test['test_nvar'],res_test['returns'],(1-cl))\n",
        "  #bt_test_n.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood on test set: Normal\")\n",
        "  print(bt_test_n.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Normal\")\n",
        "  print(bt_test_n.dq_bt())\n",
        "  bt_test_g=Backtest(res_test['test_gvar'],res_test['returns'],(1-cl))\n",
        "  #bt_test_g.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood on test set: Gumbel\")\n",
        "  print(bt_test_g.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: Gumbel\")\n",
        "  print(bt_test_g.dq_bt())\n",
        "  bt_test_g=Backtest(res_test['test_HS'],res_test['returns'],(1-cl))\n",
        "  #bt_test_g.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood on test set: HS\")\n",
        "  print(bt_test_g.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli) on test set: HS\")\n",
        "  print(bt_test_g.dq_bt())\n",
        " \n",
        "  \"\"\"\n",
        "  ########\n",
        "  \n",
        "  res['Prediction']=None\n",
        "  #res['Prediction']=-train_predictions\n",
        "  res['Prediction']=-train_predictions\n",
        "  plt.subplot(111)\n",
        "  plt.plot(res['Prediction'],c='b')\n",
        "  plt.plot(-res['Prediction'],c='b')\n",
        "  plt.plot(res['Return'],c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['NVaR'][nh:n], label = 'NVaR',c='c')\n",
        "  plt.plot(-df['NVaR'][nh:n],c='c')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['TVaR'][nh:n], label = 'TVaR',c='red')\n",
        "  plt.plot(-df['TVaR'][nh:n],c='red')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['logVaR'][nh:n], label = 'logVaR',c='y')\n",
        "  plt.plot(-df['logVaR'][nh:n],c='y')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        " \n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['FVaR'][nh:n], label = 'FVaR',c='y')\n",
        "  plt.plot(-df['FVaR'][nh:n],c='y')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        " \n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['GVaR'][nh:n], label = 'GVaR',c='y')\n",
        "  plt.plot(-df['GVaR'][nh:n],c='y')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['HS'][nh:n], label = 'HS',c='y')\n",
        "  plt.plot(-df['HS'][nh:n],c='y')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  plt.subplot(111)\n",
        "  plt.plot(df['FHS'][nh:n], label = 'FHS',c='y')\n",
        "  plt.plot(-df['FHS'][nh:n],c='y')\n",
        "  plt.plot(df['d_return'][nh:n], label = 'Return',c='g')\n",
        "  _=plt.legend()\n",
        "  plt.show()\n",
        " \n",
        " \n",
        " \n",
        "  exceptionsn=0\n",
        "  for i in range(nh,n):\n",
        "    if (df['NVaR'][i]<-df['d_return'][i+1]):\n",
        "      exceptionsn = exceptionsn+1\n",
        "\n",
        "  exceptionst=0\n",
        "  for i in range(nh,n):\n",
        "    if (df['TVaR'][i]<-df['d_return'][i+1]):\n",
        "      exceptionst = exceptionst+1\n",
        "\n",
        "  exceptionsl=0\n",
        "  for i in range(nh,n):\n",
        "    if (df['logVaR'][i]<-df['d_return'][i+1]):\n",
        "      exceptionsl = exceptionsl+1\n",
        "\n",
        "\n",
        "  #exceptionsm=0\n",
        "  #for i in range(nh,nh+len(res['Prediction'])-1):\n",
        "   # if (res['Prediction'][i]>res['Return'][i+1]):\n",
        "    #  exceptionsm = exceptionsm+1\n",
        "\n",
        "  #print(\"Exceptions Model \"+str(exceptionsm))\n",
        "  #print(\"Exceptions NVaR \"+str(exceptionsn))\n",
        "  #print(\"Exceptions TVaR \"+str(exceptionst))\n",
        "  #print(\"Exceptions logVaR \"+str(exceptionsl))\n",
        "\n",
        "  bt=Backtest(res['Return'],res['Prediction'],(1-cl))\n",
        "  [kupiec,kupiecp_model_is]=bt.lr_bt()\n",
        "  #if(kupiecp_model_is!=0):\n",
        "    #bt.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt.dq_bt())\n",
        "  kupiec_c_model_is=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_model_is=kupiec[\"p-value\"][0]\n",
        "  kupiecp_model_is=kupiecp_model_is\n",
        "  \n",
        "  \n",
        "  res['Prediction_wo_aor']=None\n",
        "  res['Prediction_wo_aor']=-train_predictions_wo_aor\n",
        "  bt_wo_aor=Backtest(res['Return'],res['Prediction_wo_aor'],(1-cl))\n",
        "  [kupiec,kupiecp_model_is]=bt_wo_aor.lr_bt()\n",
        "  #if(kupiecp_model_is!=0):\n",
        "    #bt.plot()\n",
        "  print(\"Kupiec-Christoffersen loglikelihood framework\")\n",
        "  print(bt_wo_aor.lr_bt())\n",
        "  print(\"Dynamic Quantile Test (Engle & Manganelli)\")\n",
        "  print(bt_wo_aor.dq_bt())\n",
        "  kupiec_c_model_wo_aor_is=kupiec[\"p-value\"][2]\n",
        "  kupiec_uc_model_wo_aor_is=kupiec[\"p-value\"][0]\n",
        "  kupiecp_model_wo_aor_is=kupiecp_model_is\n",
        "  \n",
        "  return res,nh,df,kupiec_c_model_is,kupiec_uc_model_is,kupiecp_model_is,kupiec_c_model_wo_aor_is,kupiec_uc_model_wo_aor_is,kupiecp_model_wo_aor_is,kupiec_c_model_oos,kupiec_uc_model_oos,kupiecp_model_oos,kupiec_c_model_wo_aor_oos,kupiec_uc_model_wo_aor_oos,kupiecp_model_wo_aor_oos,kupiec_c_normal_oos,kupiec_uc_normal_oos,kupiecp_normal_oos,kupiec_c_gumbel_oos,kupiec_uc_gumbel_oos,kupiecp_gumbel_oos,kupiec_c_hs_oos,kupiec_uc_hs_oos,kupiecp_hs_oos,kupiec_c_fhs_oos,kupiec_uc_fhs_oos,kupiecp_fhs_oos,kupiec_c_fvar_oos,kupiec_uc_fvar_oos,kupiecp_fvar_oos,kupiec_c_logvar_oos,kupiec_uc_logvar_oos,kupiecp_logvar_oos\n",
        "  \n",
        "\n",
        "################################################################################\n",
        "##### MAIN CODE: EXTRACTION OF DATA AND SUMMARY DATAFRAME                  #####\n",
        "################################################################################\n",
        "\n",
        "from google.colab import files\n",
        "penalty=0 #rmse loss\n",
        "count=0\n",
        "\n",
        "\n",
        "nr_of_generations = 10 #@param {type:\"integer\"}\n",
        "population_size = 10 #@param {type:\"integer\"}\n",
        "startticker = 20 #@param {type:\"integer\"}\n",
        "count=startticker\n",
        "endticker = 50 #@param {type:\"integer\"}\n",
        "penalty = 10 #@param {type:\"integer\"}\n",
        "eikon_user_id =\"\" #@param {type:\"string\"}\n",
        "eikon_password =\"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "DWE=Datastream(username=eikon_user_id, password=eikon_password)\n",
        "\n",
        "countries = ['BG','CN','FR','BD','IT','JP','NL','SD','SW','UK','US']\n",
        "industries = ['OILGS','BMATR','INDUS','CNSMG','HLTHC','CNSMS','TELCM','UTILS','FINAN','TECNO']\n",
        "#countries = ['AU','CN','FR','BD','IT','JP','NL','ES','SD','SW','UK','US'] #ZONDER BG\n",
        "#industries = ['OILGS','BMATR','INDUS','HLTHC''CNSMG','CNSMS','TELCM','UTILS','FINAN','TECNO'] \n",
        "#countries = ['BG']\n",
        "#industries = ['OILGS']\n",
        "\n",
        "namelist =[]\n",
        "countrylist=[]\n",
        "industrylist=[]\n",
        "for c in countries:\n",
        "  for i in industries:\n",
        "    res = DWE.get_constituents(i+c)\n",
        "    for a in res['IBTKR'][:10]:\n",
        "      namelist.append(a)\n",
        "      countrylist.append(c)\n",
        "      industrylist.append(i)\n",
        "crosssectionaldata = pd.DataFrame(data={'Ticker':namelist,'Country':countrylist,'Industry':industrylist})\n",
        "\n",
        "tickerlist=crosssectionaldata['Ticker']\n",
        "countrylists=crosssectionaldata['Country']\n",
        "industrylists=crosssectionaldata['Industry']\n",
        "\n",
        "\n",
        "summ = pd.DataFrame(data=None, dtype=None)\n",
        "summ['Ticker']=tickerlist\n",
        "summ['Country']=countrylists\n",
        "summ['Industry']=industrylists\n",
        "summ['Average 10-day MVaR']=None\n",
        "summ['Average 10-day Return']=None\n",
        "summ['Performance ratio']=None\n",
        "summ['Average Higushi fractal dimension']=None\n",
        "summ['Average Katz fractal dimension']=None\n",
        "summ['Average Hurst Exponent']=None\n",
        "summ['UR rough model IS']=None\n",
        "summ['CR rough model IS']=None\n",
        "summ['Violation ratio rough model IS']=None\n",
        "summ['UR combination model IS']=None\n",
        "summ['CR combination model IS']=None\n",
        "summ['Violation ratio combination model IS']=None\n",
        "summ['UR rough model OOS']=None\n",
        "summ['CR rough model OOS']=None\n",
        "summ['Violation ratio rough model OOS']=None\n",
        "summ['UR combination model OOS']=None\n",
        "summ['CR combination model OOS']=None\n",
        "summ['Violation ratio combination model OOS']=None\n",
        "summ['UR normal model OOS']=None\n",
        "summ['CR normal model OOS']=None\n",
        "summ['Violation ratio normal model OOS']=None\n",
        "summ['UR Gumbel model OOS']=None\n",
        "summ['CR Gumbel model OOS']=None\n",
        "summ['Violation ratio Gumbel model OOS']=None\n",
        "summ['UR HS model OOS']=None\n",
        "summ['CR HS model OOS']=None\n",
        "summ['Violation ratio HS model OOS']=None\n",
        "summ['UR FHS model OOS']=None\n",
        "summ['CR FHS model OOS']=None\n",
        "summ['Violation ratio FHS model OOS']=None\n",
        "summ['UR Fréchet model OOS']=None\n",
        "summ['CR Fréchet model OOS']=None\n",
        "summ['Violation ratio Fréchet model OOS']=None\n",
        "summ['UR lognormal model OOS']=None\n",
        "summ['CR lognormal model OOS']=None\n",
        "summ['Violation ratio lognormal model OOS']=None\n",
        "\n",
        "#summ['Average Shannon Entropy']=None\n",
        "#result=None\n",
        "for i in tickerlist[startticker:endticker]:\n",
        "#for i in tickerlist:\n",
        "  print(\"Iteration: \"+str(count))\n",
        "  if(i!='NA'):\n",
        "    res,nh,df,kupiec_c_model_is,kupiec_uc_model_is,kupiecp_model_is,kupiec_c_model_wo_aor_is,kupiec_uc_model_wo_aor_is,kupiecp_model_wo_aor_is,kupiec_c_model_oos,kupiec_uc_model_oos,kupiecp_model_oos,kupiec_c_model_wo_aor_oos,kupiec_uc_model_wo_aor_oos,kupiecp_model_wo_aor_oos,kupiec_c_normal_oos,kupiec_uc_normal_oos,kupiecp_normal_oos,kupiec_c_gumbel_oos,kupiec_uc_gumbel_oos,kupiecp_gumbel_oos,kupiec_c_hs_oos,kupiec_uc_hs_oos,kupiecp_hs_oos,kupiec_c_fhs_oos,kupiec_uc_fhs_oos,kupiecp_fhs_oos,kupiec_c_fvar_oos,kupiec_uc_fvar_oos,kupiecp_fvar_oos,kupiec_c_logvar_oos,kupiec_uc_logvar_oos,kupiecp_logvar_oos = do(i,count, nr_of_generations, population_size)\n",
        "    summ['Ticker'][count]=tickerlist[count]\n",
        "    summ['Average 10-day MVaR'][count]=(-np.mean(res['Prediction'])*100*np.sqrt(10))\n",
        "    summ['Average 10-day Return'][count]=(np.mean(res['Return'])*100*np.sqrt(10))\n",
        "    summ['Performance ratio'][count]=(np.mean(res['Return']))/(-np.mean(res['Prediction']))*100\n",
        "    summ['Average Higushi fractal dimension'][count]=np.mean(df[\"Higushi_FD_\"+str(nh)][nh:])\n",
        "    summ['Average Katz fractal dimension'][count]=np.mean(df[\"Katz_\"+str(nh)][nh:])\n",
        "    summ['Average Hurst Exponent'][count]=np.mean(df[\"Hurst_\"+str(nh)][nh:])\n",
        "    summ['UR rough model IS'][count]=kupiec_uc_model_is\n",
        "    summ['CR rough model IS'][count]=kupiec_c_model_is\n",
        "    summ['Violation ratio rough model IS'][count]=kupiecp_model_is\n",
        "    summ['UR combination model IS'][count]=kupiec_uc_model_wo_aor_is\n",
        "    summ['CR combination model IS'][count]=kupiec_c_model_wo_aor_is\n",
        "    summ['Violation ratio combination model IS'][count]=kupiecp_model_wo_aor_is\n",
        "    summ['UR rough model OOS'][count]=kupiec_uc_model_oos\n",
        "    summ['CR rough model OOS'][count]=kupiec_c_model_oos\n",
        "    summ['Violation ratio rough model OOS'][count]=kupiecp_model_oos\n",
        "    summ['UR combination model OOS'][count]=kupiec_uc_model_wo_aor_oos\n",
        "    summ['CR combination model OOS'][count]=kupiec_c_model_wo_aor_oos\n",
        "    summ['Violation ratio combination model OOS'][count]=kupiecp_model_wo_aor_oos\n",
        "    summ['UR normal model OOS'][count]=kupiec_uc_normal_oos\n",
        "    summ['CR normal model OOS'][count]=kupiec_c_normal_oos\n",
        "    summ['Violation ratio normal model OOS'][count]=kupiecp_normal_oos\n",
        "    summ['UR Gumbel model OOS'][count]=kupiec_uc_gumbel_oos\n",
        "    summ['CR Gumbel model OOS'][count]=kupiec_c_gumbel_oos\n",
        "    summ['Violation ratio Gumbel model OOS'][count]=kupiecp_gumbel_oos\n",
        "    summ['UR HS model OOS'][count]=kupiec_uc_hs_oos\n",
        "    summ['CR HS model OOS'][count]=kupiec_c_hs_oos\n",
        "    summ['Violation ratio HS model OOS'][count]=kupiecp_hs_oos\n",
        "    summ['UR FHS model OOS'][count]=kupiec_uc_fhs_oos\n",
        "    summ['CR FHS model OOS'][count]=kupiec_c_fhs_oos\n",
        "    summ['Violation ratio FHS model OOS'][count]=kupiecp_fhs_oos\n",
        "    summ['UR Fréchet model OOS'][count]=kupiec_uc_fvar_oos\n",
        "    summ['CR Fréchet model OOS'][count]=kupiec_c_fvar_oos\n",
        "    summ['Violation ratio Fréchet model OOS'][count]=kupiecp_fvar_oos\n",
        "    summ['UR lognormal model OOS'][count]=kupiec_uc_logvar_oos\n",
        "    summ['CR lognormal model OOS'][count]=kupiec_c_logvar_oos\n",
        "    summ['Violation ratio lognormal model OOS'][count]=kupiecp_logvar_oos\n",
        "    #summ['Average Shannon Entropy'][count]=np.mean(df[\"Shannon_\"+str(nh)][nh:])\n",
        "\n",
        "  count=count+1\n",
        "\n",
        "display.display(summ)\n",
        "#summ.to_csv(\"summary.csv\")\n",
        "\n",
        "df=summ[startticker:endticker]\n",
        "for i in range(startticker,endticker):\n",
        "#for i in range(startticker,7):\n",
        "#for i in range(0,len(df['Country'])-1):\n",
        "  if df['Country'][i]=='JP':\n",
        "    df['Country'][i]='Japan'\n",
        "  if df['Country'][i] =='NL':\n",
        "    df['Country'][i]='The Netherlands'\n",
        "  if df['Country'][i] =='SD':\n",
        "    df['Country'][i]='Sweden'\n",
        "  if df['Country'][i] =='SW':\n",
        "    df['Country'][i]='Switzerland'\n",
        "  if df['Country'][i] =='UK':\n",
        "    df['Country'][i]='UK'\n",
        "  if df['Country'][i]=='US':\n",
        "    df['Country'][i] = 'US'\n",
        "  if df['Country'][i]=='BG':\n",
        "    df['Country'][i] = 'Belgium'\n",
        "  if df['Country'][i]=='CN':\n",
        "    df['Country'][i] = 'Canada'\n",
        "  if df['Country'][i]=='FR':\n",
        "    df['Country'][i] = 'France'\n",
        "  if df['Country'][i]=='BD':\n",
        "    df['Country'][i] = 'Germany'\n",
        "  if df['Country'][i]=='IT':\n",
        "    df['Country'][i] = 'Italy'\n",
        "    \n",
        "file_name='result'+str(count)+'.csv'\n",
        "df.to_csv(file_name)\n",
        "from google.colab import files\n",
        "files.download(file_name)\n",
        "display.display(df)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYeXDf0ZljVB",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the code uses some parameters that can be adjusted by means of a ** form**.\n",
        "\n",
        "The **source** is by default datastream ('Refinitiv Eikon API'). One needs to fill in an eikon_user_id (e.g. DS:ABCD007) and a eikon_password (e.g. DUMBO123) in order for it to function. Alternatively, there are some other APIs included, but these are less stable and the code needs some minor adjustments for it to function.\n",
        "\n",
        "The **starting** and **ending date** speak for themselves and the user is of course free to play with these.\n",
        "\n",
        "The **confidence level** is the threshold for the market risk measure (i.e. a 99% cl means that the model will 'underpredict' a financial loss with a 1% probability).\n",
        "The **hurst_n** and **lookback_period** are two user defined time windows that are used for the calibration of the Hurst exponent and standard models respectively. (Note that these parameters are explained in the main text)\n",
        "\n",
        "The **nr_of_generations** and **population_size** are the two parameters needed for our genetic algorithms (see main text). These algorithms are now 'turned off' (i.e. put in comments), since they are very time-consuming and if only for development or demo purposes.\n",
        "\n",
        "The **start** and **endticker** is the index of a ticker within our stock universe (ranging from 0 to 870). These parameters allow us to train the models for only a range of tickers instead of all 870 consecutively.\n",
        "\n",
        "Lastly, the **penalty** variable is a tunable parameter in our custom loss function. It determines how much weigth is given to the exception loss (violation ratio defers from the significance level) versus standard L2 loss (predicted VaR compared to hypothetical real VaR).\n",
        "\n"
      ]
    }
  ]
}